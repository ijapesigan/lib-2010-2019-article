@Article{Biesanz-Falk-Savalei-2010,
  author = {Jeremy C. Biesanz and Carl F. Falk and Victoria Savalei},
  date = {2010-08},
  journaltitle = {Multivariate Behavioral Research},
  title = {Assessing mediational models: Testing and interval estimation for indirect effects},
  doi = {10.1080/00273171.2010.498292},
  number = {4},
  pages = {661--701},
  volume = {45},
  abstract = {Theoretical models specifying indirect or mediated effects are common in the social sciences. An indirect effect exists when an independent variable's influence on the dependent variable is mediated through an intervening variable. Classic approaches to assessing such mediational hypotheses (Baron \& Kenny, 1986; Sobel, 1982) have in recent years been supplemented by computationally intensive methods such as bootstrapping, the distribution of the product methods, and hierarchical Bayesian Markov chain Monte Carlo (MCMC) methods. These different approaches for assessing mediation are illustrated using data from Dunn, Biesanz, Human, and Finn (2007). However, little is known about how these methods perform relative to each other, particularly in more challenging situations, such as with data that are incomplete and/or nonnormal. This article presents an extensive Monte Carlo simulation evaluating a host of approaches for assessing mediation. We examine Type I error rates, power, and coverage. We study normal and nonnormal data as well as complete and incomplete data. In addition, we adapt a method, recently proposed in statistical literature, that does not rely on confidence intervals (CIs) to test the null hypothesis of no indirect effect. The results suggest that the new inferential method--the partial posterior p value--slightly outperforms existing ones in terms of maintaining Type I error rates while maximizing power, especially with incomplete data. Among confidence interval approaches, the bias-corrected accelerated (BCa) bootstrapping approach often has inflated Type I error rates and inconsistent coverage and is not recommended. In contrast, the bootstrapped percentile confidence interval and the hierarchical Bayesian MCMC method perform best overall, maintaining Type I error rates, exhibiting reasonable power, and producing stable and accurate coverage rates.},
  publisher = {Informa {UK} Limited},
  annotation = {mediation, mediation-bootstrap, mediation-bayesian},
}

@Article{Blanca-Arnau-LopezMontiel-etal-2013,
  author = {Mar\'\iaJ. Blanca and Jaume Arnau and Dolores L{\a'o}pez-Montiel and Roser Bono and Rebecca Bendayan},
  date = {2013-05},
  journaltitle = {Methodology},
  title = {Skewness and kurtosis in real data samples},
  doi = {10.1027/1614-2241/a000057},
  number = {2},
  pages = {78--84},
  volume = {9},
  abstract = {Parametric statistics are based on the assumption of normality. Recent findings suggest that Type I error and power can be adversely affected when data are non-normal. This paper aims to assess the distributional shape of real data by examining the values of the third and fourth central moments as a measurement of skewness and kurtosis in small samples. The analysis concerned 693 distributions with a sample size ranging from 10 to 30. Measures of cognitive ability and of other psychological variables were included. The results showed that skewness ranged between -2.49 and 2.33. The values of kurtosis ranged between -1.92 and 7.41. Considering skewness and kurtosis together the results indicated that only 5.5\% of distributions were close to expected values under normality. Although extreme contamination does not seem to be very frequent, the findings are consistent with previous research suggesting that normality is not the rule with real data.},
  publisher = {Hogrefe Publishing Group},
}

@Article{Boettiger-Eddelbuettel-2017,
  author = {Carl Boettiger and Dirk Eddelbuettel},
  date = {2017},
  journaltitle = {The R Journal},
  title = {An introduction to {Rocker}: Docker containers for {R}},
  doi = {10.32614/rj-2017-065},
  number = {2},
  pages = {527},
  volume = {9},
  abstract = {We describe the Rocker project, which provides a widely-used suite of Docker images with customized R environments for particular tasks. We discuss how this suite is organized, and how these tools can increase portability, scaling, reproducibility, and convenience of R users and developers.},
  publisher = {The R Foundation},
  annotation = {container, container-docker, container-docker-rocker},
}

@Article{Chow-Ho-Hamaker-etal-2010,
  author = {Sy-Miin Chow and Moon-ho R. Ho and Ellen L. Hamaker and Conor V. Dolan},
  date = {2010-04},
  journaltitle = {Structural Equation Modeling: A Multidisciplinary Journal},
  title = {Equivalence and differences between structural equation modeling and state-space modeling techniques},
  doi = {10.1080/10705511003661553},
  number = {2},
  pages = {303--332},
  volume = {17},
  abstract = {State-space modeling techniques have been compared to structural equation modeling (SEM) techniques in various contexts but their unique strengths have often been overshadowed by their similarities to SEM. In this article, we provide a comprehensive discussion of these 2 approaches' similarities and differences through analytic comparisons and numerical simulations, with a focus on their use in representing intraindividual dynamics and interindividual differences. To demonstrate the respective strengths and weaknesses of the 2 approaches in representing these 2 aspects, we simulated data under (a) a cross-sectional common factor model, (b) a latent difference score model with random effects in intercept and slope, and (c) a bivariate dynamic factor analysis model with auto- and cross-regression parameters. Possible ways in which SEM and state-space modeling can be utilized as complementary tools in representing human developmental and other related processes are discussed.},
  publisher = {Informa {UK} Limited},
  annotation = {ild, sem, ssm},
}

@Article{Deboeck-Preacher-2015,
  author = {Pascal R. Deboeck and Kristopher J. Preacher},
  date = {2015-06},
  journaltitle = {Structural Equation Modeling: A Multidisciplinary Journal},
  title = {No Need to be Discrete: A Method for Continuous Time Mediation Analysis},
  doi = {10.1080/10705511.2014.973960},
  number = {1},
  pages = {61--75},
  volume = {23},
  abstract = {Mediation is one concept that has shaped numerous theories. The list of problems associated with mediation models, however, has been growing. Mediation models based on cross-sectional data can produce unexpected estimates, so much so that making longitudinal or causal inferences is inadvisable. Even longitudinal mediation models have faults, as parameter estimates produced by these models are specific to the lag between observations, leading to much debate over appropriate lag selection. Using continuous time models (CTMs) rather than commonly employed discrete time models, one can estimate lag-independent parameters. We demonstrate methodology that allows for continuous time mediation analyses, with attention to concepts such as indirect and direct effects, partial mediation, the effect of lag, and the lags at which relations become maximal. A simulation compares common longitudinal mediation methods with CTMs. Reanalysis of a published covariance matrix demonstrates that CTMs can be fit to data used in longitudinal mediation studies.},
  publisher = {Informa {UK} Limited},
  keywords = {continuous time models, cross-lagged panel model, exact discrete model, longitudinal mediation, mediation},
  annotation = {mediation, mediation-longitudinal},
}

@Article{Dudgeon-2017,
  author = {Paul Dudgeon},
  date = {2017-03},
  journaltitle = {Psychometrika},
  title = {Some improvements in confidence intervals for standardized regression coefficients},
  doi = {10.1007/s11336-017-9563-z},
  number = {4},
  pages = {928--951},
  volume = {82},
  keywords = {standardized regression coefficients, robust confidence intervals, non-normality},
  abstract = {Yuan and Chan (Psychometrika 76:670–690, 2011. doi:10.1007/S11336-011-9224-6) derived consistent confidence intervals for standardized regression coefficients under fixed and random score assumptions. Jones and Waller (Psychometrika 80:365–378, 2015. doi:10.1007/S11336-013-9380-Y) extended these developments to circumstances where data are non-normal by examining confidence intervals based on Browne's (Br J Math Stat Psychol 37:62–83, 1984. doi:10.1111/j.2044-8317.1984.tb00789.x) asymptotic distribution-free (ADF) theory. Seven different heteroscedastic-consistent (HC) estimators were investigated in the current study as potentially better solutions for constructing confidence intervals on standardized regression coefficients under non-normality. Normal theory, ADF, and HC estimators were evaluated in a Monte Carlo simulation. Findings confirmed the superiority of the HC3 (MacKinnon and White, J Econ 35:305–325, 1985. doi:10.1016/0304-4076(85)90158-7) and HC5 (Cribari-Neto and Da Silva, Adv Stat Anal 95:129–146, 2011. doi:10.1007/s10182-010-0141-2) interval estimators over Jones and Waller's ADF estimator under all conditions investigated, as well as over the normal theory method. The HC5 estimator was more robust in a restricted set of conditions over the HC3 estimator. Some possible extensions of HC estimators to other effect size measures are considered for future developments.},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Eddelbuettel-Balamuta-2017,
  author = {Dirk Eddelbuettel and James Joseph Balamuta},
  date = {2017-08},
  journaltitle = {PeerJ Preprints},
  title = {Extending {R} with {C++}: A brief introduction to {Rcpp}},
  doi = {10.7287/peerj.preprints.3188v1},
  number = {3},
  volume = {3188v1},
  abstract = {R has always provided an application programming interface (API) for extensions. Based on the C language, it uses a number of macros and other low-level constructs to exchange data structures between the R process and any dynamically-loaded component modules authors added to it. With the introduction of the Rcpp package, and its later refinements, this process has become considerably easier yet also more robust. By now, Rcpp has become the most popular extension mechanism for R. This article introduces Rcpp, and illustrates with several examples how the Rcpp Attributes mechanism in particular eases the transition of objects between R and C++ code.},
  publisher = {{PeerJ}},
  annotation = {r, r-packages},
}

@Article{Eddelbuettel-Francois-2011,
  author = {Dirk Eddelbuettel and Romain Fran{\c c}ois},
  date = {2011},
  journaltitle = {Journal of Statistical Software},
  title = {{Rcpp}: Seamless {R} and {C++} integration},
  doi = {10.18637/jss.v040.i08},
  number = {8},
  volume = {40},
  abstract = {The Rcpp package simplifies integrating C++ code with R. It provides a consistent C++ class hierarchy that maps various types of R objects (vectors, matrices, functions, environments, ...) to dedicated C++ classes. Object interchange between R and C++ is managed by simple, flexible and extensible concepts which include broad support for C++ Standard Template Library idioms. C++ code can both be compiled, linked and loaded on the fly, or added via packages. Flexible error and exception code handling is provided. Rcpp substantially lowers the barrier for programmers wanting to combine C++ code with R.},
  publisher = {Foundation for Open Access Statistic},
  annotation = {r, r-packages},
}

@Article{Eddelbuettel-Sanderson-2014,
  author = {Dirk Eddelbuettel and Conrad Sanderson},
  date = {2014-03},
  journaltitle = {Computational Statistics \& Data Analysis},
  title = {{RcppArmadillo}: Accelerating {R} with high-performance {C++} linear algebra},
  doi = {10.1016/j.csda.2013.02.005},
  pages = {1054--1063},
  volume = {71},
  abstract = {The R statistical environment and language has demonstrated particular strengths for interactive development of statistical algorithms, as well as data modelling and visualisation. Its current implementation has an interpreter at its core which may result in a performance penalty in comparison to directly executing user algorithms in the native machine code of the host CPU. In contrast, the C++ language has no built-in visualisation capabilities, handling of linear algebra or even basic statistical algorithms; however, user programs are converted to high-performance machine code, ahead of execution. A new method avoids possible speed penalties in R by using the Rcpp extension package in conjunction with the Armadillo C++ matrix library. In addition to the inherent performance advantages of compiled code, Armadillo provides an easy-to-use template-based meta-programming framework, allowing the automatic pooling of several linear algebra operations into one, which in turn can lead to further speedups. With the aid of Rcpp and Armadillo, conversion of linear algebra centred algorithms from R to C++ becomes straightforward. The algorithms retain the overall structure as well as readability, all while maintaining a bidirectional link with the host R environment. Empirical timing comparisons of R and C++ implementations of a Kalman filtering algorithm indicate a speedup of several orders of magnitude.},
  publisher = {Elsevier {BV}},
  annotation = {r, r-packages},
}

@Article{Fritz-Taylor-MacKinnon-2012,
  author = {Matthew S. Fritz and Aaron B. Taylor and David P. MacKinnon},
  date = {2012-02},
  journaltitle = {Multivariate Behavioral Research},
  title = {Explanation of two anomalous results in statistical mediation analysis},
  doi = {10.1080/00273171.2012.640596},
  number = {1},
  pages = {61--87},
  volume = {47},
  abstract = {Previous studies of different methods of testing mediation models have consistently found two anomalous results. The first result is elevated Type I error rates for the bias-corrected and accelerated bias-corrected bootstrap tests not found in nonresampling tests or in resampling tests that did not include a bias correction. This is of special concern as the bias-corrected bootstrap is often recommended and used due to its higher statistical power compared with other tests. The second result is statistical power reaching an asymptote far below 1.0 and in some conditions even declining slightly as the size of the relationship between X and M, a, increased. Two computer simulations were conducted to examine these findings in greater detail. Results from the first simulation found that the increased Type I error rates for the bias-corrected and accelerated bias-corrected bootstrap are a function of an interaction between the size of the individual paths making up the mediated effect and the sample size, such that elevated Type I error rates occur when the sample size is small and the effect size of the nonzero path is medium or larger. Results from the second simulation found that stagnation and decreases in statistical power as a function of the effect size of the a path occurred primarily when the path between M and Y, b, was small. Two empirical mediation examples are provided using data from a steroid prevention and health promotion program aimed at high school football players (Athletes Training and Learning to Avoid Steroids; Goldberg et al., 1996), one to illustrate a possible Type I error for the bias-corrected bootstrap test and a second to illustrate a loss in power related to the size of a. Implications of these findings are discussed.},
  publisher = {Informa {UK} Limited},
  annotation = {mediation, mediation-bootstrap},
}

@Article{Hayes-Scharkow-2013,
  author = {Andrew F. Hayes and Michael Scharkow},
  date = {2013-08},
  journaltitle = {Psychological Science},
  title = {The relative trustworthiness of inferential tests of the indirect effect in statistical mediation analysis},
  doi = {10.1177/0956797613480187},
  number = {10},
  pages = {1918--1927},
  volume = {24},
  abstract = {A content analysis of 2 years of Psychological Science articles reveals inconsistencies in how researchers make inferences about indirect effects when conducting a statistical mediation analysis. In this study, we examined the frequency with which popularly used tests disagree, whether the method an investigator uses makes a difference in the conclusion he or she will reach, and whether there is a most trustworthy test that can be recommended to balance practical and performance considerations. We found that tests agree much more frequently than they disagree, but disagreements are more common when an indirect effect exists than when it does not. We recommend the bias-corrected bootstrap confidence interval as the most trustworthy test if power is of utmost concern, although it can be slightly liberal in some circumstances. Investigators concerned about Type I errors should choose the Monte Carlo confidence interval or the distribution-of-the-product approach, which rarely disagree. The percentile bootstrap confidence interval is a good compromise test.},
  publisher = {{SAGE} Publications},
  annotation = {mediation, mediation-bootstrap, mediation-montecarlo, mediation-prodclin},
}

@Article{Hunter-2017,
  author = {Michael D. Hunter},
  date = {2017-10},
  journaltitle = {Structural Equation Modeling: A Multidisciplinary Journal},
  title = {State Space Modeling in an Open Source, Modular, Structural Equation Modeling Environment},
  doi = {10.1080/10705511.2017.1369354},
  number = {2},
  pages = {307--324},
  volume = {25},
  abstract = {State space models (SSMs) are introduced in the context of structural equation modeling (SEM). In particular, the OpenMx implementation of SSMs using the Kalman filter and prediction error decomposition is discussed. In reflection of modularity, the implementation uses the same full information maximum likelihood missing data procedures for SSMs and SEMs. Similarly, generic OpenMx features such as likelihood ratio tests, profile likelihood confidence intervals, Hessian-based standard errors, definition variables, and the matrix algebra interface are all supported. Example scripts for specification of autoregressive models, multiple lag models (VAR(p)), multiple lag moving average models (VARMA(p, q)), multiple subject models, and latent growth models are provided. Additionally, latent variable calculation based on the Kalman filter and raw data generation based on a model are all included. Finally, future work for extending SSMs to allow for random effects and for presenting them in diagrams is discussed.},
  publisher = {Informa {UK} Limited},
  keywords = {state space model, software, Kalman filter, OpenMx},
  annotation = {ild, ild-software, sem, sem-software, ssm, ssm-software},
}

@Article{Jones-Waller-2013a,
  author = {Jeff A. Jones and Niels G. Waller},
  date = {2013},
  journaltitle = {Psychological Methods},
  title = {Computing confidence intervals for standardized regression coefficients.},
  doi = {10.1037/a0033269},
  number = {4},
  pages = {435--453},
  volume = {18},
  abstract = {With fixed predictors, the standard method (Cohen, Cohen, West, \& Aiken, 2003, p. 86; Harris, 2001, p. 80; Hays, 1994, p. 709) for computing confidence intervals (CIs) for standardized regression coefficients fails to account for the sampling variability of the criterion standard deviation. With random predictors, this method also fails to account for the sampling variability of the predictor standard deviations. Nevertheless, under some conditions the standard method will produce CIs with accurate coverage rates. To delineate these conditions, we used a Monte Carlo simulation to compute empirical CI coverage rates in samples drawn from 36 populations with a wide range of data characteristics. We also computed the empirical CI coverage rates for 4 alternative methods that have been discussed in the literature: noncentrality interval estimation, the delta method, the percentile bootstrap, and the bias-corrected and accelerated bootstrap. Our results showed that for many data-parameter configurations--for example, sample size, predictor correlations, coefficient of determination ($R^2$), orientation of $\beta$ with respect to the eigenvectors of the predictor correlation matrix, $R_X$--the standard method produced coverage rates that were close to their expected values. However, when population $R^2$ was large 	and when $\beta$ approached the last eigenvector of $R_X$, then the standard method coverage rates were frequently below the nominal rate (sometimes by a considerable amount). In these conditions, the delta method and the 2 bootstrap procedures were consistently accurate. Results using noncentrality interval estimation were inconsistent. In light of these findings, we recommend that researchers use the delta method to evaluate the sampling variability of standardized regression coefficients.},
  publisher = {American Psychological Association ({APA})},
}

@Article{Jones-Waller-2015,
  author = {Jeff A. Jones and Niels G. Waller},
  date = {2015-06},
  journaltitle = {Psychometrika},
  title = {The Normal-Theory and Asymptotic Distribution-Free ({ADF}) Covariance Matrix of Standardized Regression Coefficients: Theoretical Extensions and Finite Sample Behavior},
  doi = {10.1007/s11336-013-9380-y},
  number = {2},
  pages = {365--378},
  volume = {80},
  abstract = {Yuan and Chan (Psychometrika, 76, 670–690, 2011) recently showed how to compute the covariance matrix of standardized regression coefficients from covariances. In this paper, we describe a method for computing this covariance matrix from correlations. Next, we describe an asymptotic distribution-free (ADF; Browne in British Journal of Mathematical and Statistical Psychology, 37, 62–83, 1984) method for computing the covariance matrix of standardized regression coefficients. We show that the ADF method works well with nonnormal data in moderate-to-large samples using both simulated and real-data examples. R code (R Development Core Team, 2012) is available from the authors or through the Psychometrika online repository for supplementary materials.},
  publisher = {Springer Science and Business Media {LLC}},
  annotation = {standardized-regression, standardized-regression-hc},
}

@Article{KisbuSakarya-MacKinnon-Miocevic-2014,
  author = {Yasemin Kisbu-Sakarya and David P. MacKinnon and Milica Mio{\v c}evi{\a'c}},
  date = {2014-05},
  journaltitle = {Multivariate Behavioral Research},
  title = {The distribution of the product explains normal theory mediation confidence interval estimation},
  doi = {10.1080/00273171.2014.903162},
  number = {3},
  pages = {261--268},
  volume = {49},
  abstract = {The distribution of the product has several useful applications. One of these applications is its use to form confidence intervals for the indirect effect as the product of 2 regression coefficients. The purpose of this article is to investigate how the moments of the distribution of the product explain normal theory mediation confidence interval coverage and imbalance. Values of the critical ratio for each random variable are used to demonstrate how the moments of the distribution of the product change across values of the critical ratio observed in research studies. Results of the simulation study showed that as skewness in absolute value increases, coverage decreases. And as skewness in absolute value and kurtosis increases, imbalance increases. The difference between testing the significance of the indirect effect using the normal theory versus the asymmetric distribution of the product is further illustrated with a real data example. This article is the first study to show the direct link between the distribution of the product and indirect effect confidence intervals and clarifies the results of previous simulation studies by showing why normal theory confidence intervals for indirect effects are often less accurate than those obtained from the asymmetric distribution of the product or from resampling methods.},
  publisher = {Informa {UK} Limited},
  annotation = {mediation, mediation-prodclin},
}

@Article{Koopman-Howe-Hollenbeck-etal-2015,
  author = {Joel Koopman and Michael Howe and John R. Hollenbeck and Hock-Peng Sin},
  date = {2015},
  journaltitle = {Journal of Applied Psychology},
  title = {Small sample mediation testing: Misplaced confidence in bootstrapped confidence intervals},
  doi = {10.1037/a0036635},
  number = {1},
  pages = {194--202},
  volume = {100},
  abstract = {Bootstrapping is an analytical tool commonly used in psychology to test the statistical significance of the indirect effect in mediation models. Bootstrapping proponents have particularly advocated for its use for samples of 20-80 cases. This advocacy has been heeded, especially in the Journal of Applied Psychology, as researchers are increasingly utilizing bootstrapping to test mediation with samples in this range. We discuss reasons to be concerned with this escalation, and in a simulation study focused specifically on this range of sample sizes, we demonstrate not only that bootstrapping has insufficient statistical power to provide a rigorous hypothesis test in most conditions but also that bootstrapping has a tendency to exhibit an inflated Type I error rate. We then extend our simulations to investigate an alternative empirical resampling method as well as a Bayesian approach and demonstrate that they exhibit comparable statistical power to bootstrapping in small samples without the associated inflated Type I error. Implications for researchers testing mediation hypotheses in small samples are presented. For researchers wishing to use these methods in their own research, we have provided R syntax in the online supplemental materials.},
  publisher = {American Psychological Association ({APA})},
  keywords = {mediation, bootstrapping, permutation, Bayes},
  annotation = {mediation, mediation-bootstrap, mediation-bayesian},
}

@Article{Kurtzer-Sochat-Bauer-2017,
  author = {Gregory M. Kurtzer and Vanessa Sochat and Michael W. Bauer},
  date = {2017-05},
  journaltitle = {{PLOS} {ONE}},
  title = {{Singularity}: Scientific containers for mobility of compute},
  doi = {10.1371/journal.pone.0177459},
  editor = {Attila Gursoy},
  number = {5},
  pages = {e0177459},
  volume = {12},
  publisher = {Public Library of Science ({PLoS})},
  annotation = {container, container-singularity},
}

@Article{Kwan-Chan-2011,
  author = {Joyce L. Y. Kwan and Wai Chan},
  date = {2011-04},
  journaltitle = {Behavior Research Methods},
  title = {Comparing standardized coefficients in structural equation modeling: A model reparameterization approach},
  doi = {10.3758/s13428-011-0088-6},
  number = {3},
  pages = {730--745},
  volume = {43},
  abstract = {We propose a two-stage method for comparing standardized coefficients in structural equation modeling (SEM). At stage 1, we transform the original model of interest into the standardized model by model reparameterization, so that the model parameters appearing in the standardized model are equivalent to the standardized parameters of the original model. At stage 2, we impose appropriate linear equality constraints on the standardized model and use a likelihood ratio test to make statistical inferences about the equality of standardized coefficients. Unlike other existing methods for comparing standardized coefficients, the proposed method does not require specific modeling features (e.g., specification of nonlinear constraints), which are available only in certain SEM software programs. Moreover, this method allows researchers to compare two or more standardized coefficients simultaneously in a standard and convenient way. Three real examples are given to illustrate the proposed method, using EQS, a popular SEM software program. Results show that the proposed method performs satisfactorily for testing the equality of standardized coefficients.},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Kwan-Chan-2014,
  author = {Joyce L. Y. Kwan and Wai Chan},
  date = {2014-04},
  journaltitle = {Structural Equation Modeling: A Multidisciplinary Journal},
  title = {Comparing squared multiple correlation coefficients using structural equation modeling},
  doi = {10.1080/10705511.2014.882673},
  number = {2},
  pages = {225--238},
  volume = {21},
  abstract = {In social science research, a common topic in multiple regression analysis is to compare the squared multiple correlation coefficients in different populations. Existing methods based on asymptotic theories (Olkin \& Finn, 1995) and bootstrapping (Chan, 2009) are available but these can only handle a 2-group comparison. Another method based on structural equation modeling (SEM) has been proposed recently. However, this method has three disadvantages. First, it requires the user to explicitly specify the sample R2 as a function in terms of the basic SEM model parameters, which is sometimes troublesome and error prone. Second, it requires the specification of nonlinear constraints, which is not available in some popular SEM software programs. Third, it is for a 2-group comparison primarily. In this article, a 2-stage SEM method is proposed as an alternative. Unlike all other existing methods, the proposed method is simple to use, and it does not require any specific programming features such as the specification of nonlinear constraints. More important, the method allows a simultaneous comparison of 3 or more groups. A real example is given to illustrate the proposed method using EQS, a popular SEM software program.},
  keywords = {squared multiple correlation coefficients, structural equation modeling, model reparameterization, multi-sample analysis},
  publisher = {Informa {UK} Limited},
}

@Article{Merkel-2014,
  author = {Dirk Merkel},
  date = {2014},
  journaltitle = {Linux Journal},
  title = {{Docker}: Lightweight {Linux} containers for consistent development and deployment},
  number = {239},
  pages = {2},
  volume = {2014},
  url = {https://www.linuxjournal.com/content/docker-lightweight-linux-containers-consistent-development-and-deployment},
  annotation = {container, container-docker},
}

@Article{Neale-Hunter-Pritikin-etal-2015,
  author = {Michael C. Neale and Michael D. Hunter and Joshua N. Pritikin and Mahsa Zahery and Timothy R. Brick and Robert M. Kirkpatrick and Ryne Estabrook and Timothy C. Bates and Hermine H. Maes and Steven M. Boker},
  date = {2015-01},
  journaltitle = {Psychometrika},
  title = {{OpenMx} 2.0: Extended Structural Equation and Statistical Modeling},
  doi = {10.1007/s11336-014-9435-8},
  number = {2},
  pages = {535--549},
  volume = {81},
  abstract = {The new software package OpenMx 2.0 for structural equation and other statistical modeling is introduced and its features are described. OpenMx is evolving in a modular direction and now allows a mix-and-match computational approach that separates model expectations from fit functions and optimizers. Major backend architectural improvements include a move to swappable open-source optimizers such as the newly written CSOLNP. Entire new methodologies such as item factor analysis and state space modeling have been implemented. New model expectation functions including support for the expression of models in LISREL syntax and a simplified multigroup expectation function are available.  Ease-of-use improvements include helper functions to standardize model parameters and compute their Jacobian-based standard errors, access to model components through standard R \$ mechanisms, and improved tab completion from within the R Graphical User Interface.},
  publisher = {Springer Science and Business Media {LLC}},
  annotation = {r, r-packages, sem, sem-software},
}

@Article{Ou-Hunter-Chow-2019,
  author = {Lu Ou and Michael D. Hunter and Sy-Miin Chow},
  date = {2019},
  journaltitle = {The R Journal},
  title = {What's for {dynr}: A package for linear and nonlinear dynamic modeling in {R}},
  doi = {10.32614/rj-2019-012},
  number = {1},
  pages = {91},
  volume = {11},
  abstract = {Intensive longitudinal data in the behavioral sciences are often noisy, multivariate in nature, and may involve multiple units undergoing regime switches by showing discontinuities interspersed with continuous dynamics. Despite increasing interest in using linear and nonlinear differential/difference equation models with regime switches, there has been a scarcity of software packages that are fast and freely accessible. We have created an R package called dynr that can handle a broad class of linear and nonlinear discreteand continuous-time models, with regime-switching properties and linear Gaussian measurement functions, in C, while maintaining simple and easy-to learn model specification functions in R. We present the mathematical and computational bases used by the dynr R package, and present two illustrative examples to demonstrate the unique features of dynr.},
  publisher = {The R Foundation},
  annotation = {ild, ild-software, r, r-packages},
}

@Article{Preacher-Selig-2012,
  author = {Kristopher J. Preacher and James P. Selig},
  date = {2012-04},
  journaltitle = {Communication Methods and Measures},
  title = {Advantages of Monte Carlo Confidence Intervals for Indirect Effects},
  doi = {10.1080/19312458.2012.679848},
  number = {2},
  pages = {77--98},
  volume = {6},
  abstract = {Monte Carlo simulation is a useful but underutilized method of constructing confidence intervals for indirect effects in mediation analysis. The Monte Carlo confidence interval method has several distinct advantages over rival methods. Its performance is comparable to other widely accepted methods of interval construction, it can be used when only summary data are available, it can be used in situations where rival methods (e.g., bootstrapping and distribution of the product methods) are difficult or impossible, and it is not as computer-intensive as some other methods. In this study we discuss Monte Carlo confidence intervals for indirect effects, report the results of a simulation study comparing their performance to that of competing methods, demonstrate the method in applied examples, and discuss several software options for implementation in applied settings.},
  publisher = {Informa {UK} Limited},
  annotation = {mediation, mediation-montecarlo, mediation-bootstrap},
}

@Article{Rosseel-2012,
  author = {Yves Rosseel},
  date = {2012},
  journaltitle = {Journal of Statistical Software},
  title = {{lavaan}: An {R} package for structural equation modeling},
  doi = {10.18637/jss.v048.i02},
  number = {2},
  volume = {48},
  abstract = {Structural equation modeling (SEM) is a vast field and widely used by many applied researchers in the social and behavioral sciences. Over the years, many software packages for structural equation modeling have been developed, both free and commercial. However, perhaps the best state-of-the-art software packages in this field are still closed-source and/or commercial. The R package lavaan has been developed to provide applied researchers, teachers, and statisticians, a free, fully open-source, but commercial-quality package for latent variable modeling. This paper explains the aims behind the development of the package, gives an overview of its most important features, and provides some examples to illustrate how lavaan works in practice.},
  publisher = {Foundation for Open Access Statistic},
  annotation = {r, r-packages, sem, sem-software},
}

@Article{Schouten-Lugtig-Vink-2018,
  author = {Rianne Margaretha Schouten and Peter Lugtig and Gerko Vink},
  date = {2018-07},
  journaltitle = {Journal of Statistical Computation and Simulation},
  title = {Generating missing values for simulation purposes: A multivariate amputation procedure},
  doi = {10.1080/00949655.2018.1491577},
  number = {15},
  pages = {2909--2930},
  volume = {88},
  abstract = {Missing data form a ubiquitous problem in scientific research, especially since most statistical analyses require complete data. To evaluate the performance of methods dealing with missing data, researchers perform simulation studies. An important aspect of these studies is the generation of missing values in a simulated, complete data set: the amputation procedure. We investigated the methodological validity and statistical nature of both the current amputation practice and a newly developed and implemented multivariate amputation procedure. We found that the current way of practice may not be appropriate for the generation of intuitive and reliable missing data problems. The multivariate amputation procedure, on the other hand, generates reliable amputations and allows for a proper regulation of missing data problems. The procedure has additional features to generate any missing data scenario precisely as intended. Hence, the multivariate amputation procedure is an efficient method to accurately evaluate missing data methodology.},
  publisher = {Informa {UK} Limited},
  keywords = {missing data, multiple imputation, multivariate amputation, evaluation},
}

@Article{Taylor-MacKinnon-2012,
  author = {Aaron B. Taylor and David P. MacKinnon},
  date = {2012-02},
  journaltitle = {Behavior Research Methods},
  title = {Four applications of permutation methods to testing a single-mediator model},
  doi = {10.3758/s13428-011-0181-x},
  number = {3},
  pages = {806--844},
  volume = {44},
  abstract = {Four applications of permutation tests to the single-mediator model are described and evaluated in this study. Permutation tests work by rearranging data in many possible ways in order to estimate the sampling distribution for the test statistic. The four applications to mediation evaluated here are the permutation test of ab, the permutation joint significance test, and the noniterative and iterative permutation confidence intervals for ab. A Monte Carlo simulation study was used to compare these four tests with the four best available tests for mediation found in previous research: the joint significance test, the distribution of the product test, and the percentile and bias-corrected bootstrap tests. We compared the different methods on Type I error, power, and confidence interval coverage. The noniterative permutation confidence interval for ab was the best performer among the new methods. It successfully controlled Type I error, had power nearly as good as the most powerful existing methods, and had better coverage than any existing method. The iterative permutation confidence interval for ab had lower power than do some existing methods, but it performed better than any other method in terms of coverage. The permutation confidence interval methods are recommended when estimating a confidence interval is a primary concern. SPSS and SAS macros that estimate these confidence intervals are provided.},
  publisher = {Springer Science and Business Media {LLC}},
  keywords = {mediation, bootstrapping, permutation, Bayes},
  annotation = {mediation, mediation-bootstrap},
}

@Article{Tofighi-Kelley-2019,
  author = {Davood Tofighi and Ken Kelley},
  date = {2019-06},
  journaltitle = {Multivariate Behavioral Research},
  title = {Indirect effects in sequential mediation models: Evaluating methods for hypothesis testing and confidence interval formation},
  doi = {10.1080/00273171.2019.1618545},
  number = {2},
  pages = {188--210},
  volume = {55},
  abstract = {Complex mediation models, such as a two-mediator sequential model, have become more prevalent in the literature. To test an indirect effect in a two-mediator model, we conducted a large-scale Monte Carlo simulation study of the Type I error, statistical power, and confidence interval coverage rates of 10 frequentist and Bayesian confidence/credible intervals (CIs) for normally and nonnormally distributed data. The simulation included never-studied methods and conditions (e.g., Bayesian CI with flat and weakly informative prior methods, two model-based bootstrap methods, and two nonnormality conditions) as well as understudied methods (e.g., profile-likelihood, Monte Carlo with maximum likelihood standard error [MC-ML] and robust standard error [MC-Robust]). The popular BC bootstrap showed inflated Type I error rates and CI under-coverage. We recommend different methods depending on the purpose of the analysis. For testing the null hypothesis of no mediation, we recommend MC-ML, profile-likelihood, and two Bayesian methods. To report a CI, if data has a multivariate normal distribution, we recommend MC-ML, profile-likelihood, and the two Bayesian methods; otherwise, for multivariate nonnormal data we recommend the percentile bootstrap. We argue that the best method for testing hypotheses is not necessarily the best method for CI construction, which is consistent with the findings we present.},
  keywords = {indirect effect, confidence interval, sequential mediation, Bayesian credible interval},
  publisher = {Informa {UK} Limited},
  annotation = {mediation, mediation-bayesian, mediation-bootstrap, mediation-likelihood, mediation-montecarlo},
}

@Article{Tofighi-MacKinnon-2015,
  author = {Davood Tofighi and David P. MacKinnon},
  date = {2015-08},
  journaltitle = {Structural Equation Modeling: A Multidisciplinary Journal},
  title = {{Monte Carlo} confidence intervals for complex functions of indirect effects},
  doi = {10.1080/10705511.2015.1057284},
  number = {2},
  pages = {194--205},
  volume = {23},
  abstract = {One challenge in mediation analysis is to generate a confidence interval (CI) with high coverage and power that maintains a nominal significance level for any well-defined function of indirect and direct effects in the general context of structural equation modeling (SEM). This study discusses a proposed Monte Carlo extension that finds the CIs for any well-defined function of the coefficients of SEM such as the product of $k$ coefficients and the ratio of the contrasts of indirect effects, using the Monte Carlo method. Finally, we conduct a small-scale simulation study to compare CIs produced by the Monte Carlo, nonparametric bootstrap, and asymptotic-delta methods. Based on our simulation study, we recommend researchers use the Monte Carlo method to test a complex function of indirect effects.},
  keywords = {confidence interval, mediation analysis, Monte Carlo},
  publisher = {Informa {UK} Limited},
  annotation = {mediation, mediation-bootstrap, mediation-delta, mediation-montecarlo},
}

@Article{vanBuuren-GroothuisOudshoorn-2011,
  author = {Stef {van Buuren} and Karin Groothuis-Oudshoorn},
  date = {2011},
  journaltitle = {Journal of Statistical Software},
  title = {{mice}: Multivariate Imputation by Chained Equations in {R}},
  doi = {10.18637/jss.v045.i03},
  number = {3},
  volume = {45},
  abstract = {The R package mice imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice, which extends the functionality of mice 1.0 in several ways. In mice, the analysis of imputed data is made completely general, whereas the range of models under which pooling works is substantially extended. mice adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. mice can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
  publisher = {Foundation for Open Access Statistic},
  keywords = {MICE, multiple imputation, chained equations, fully conditional specification, Gibbs sampler, predictor selection, passive imputation, R},
}

@Article{Wu-Jia-2013,
  author = {Wei Wu and Fan Jia},
  date = {2013-09},
  journaltitle = {Multivariate Behavioral Research},
  title = {A new procedure to test mediation with missing data through nonparametric bootstrapping and multiple imputation},
  doi = {10.1080/00273171.2013.816235},
  number = {5},
  pages = {663--691},
  volume = {48},
  abstract = {This article proposes a new procedure to test mediation with the presence of missing data by combining nonparametric bootstrapping with multiple imputation (MI). This procedure performs MI first and then bootstrapping for each imputed data set. The proposed procedure is more computationally efficient than the procedure that performs bootstrapping first and then MI for each bootstrap sample. The validity of the procedure is evaluated using a simulation study under different sample size, missing data mechanism, missing data proportion, and shape of distribution conditions. The result suggests that the proposed procedure performs comparably to the procedure that combines bootstrapping with full information maximum likelihood under most conditions. However, caution needs to be taken when using this procedure to handle missing not-at-random or nonnormal data.},
  publisher = {Informa {UK} Limited},
  annotation = {mediation, mediation-missing, mediation-bootstrap},
}

@Article{Yuan-Chan-2011,
  author = {Ke-Hai Yuan and Wai Chan},
  date = {2011-08},
  journaltitle = {Psychometrika},
  title = {Biases and Standard Errors of Standardized Regression Coefficients},
  doi = {10.1007/s11336-011-9224-6},
  number = {4},
  pages = {670--690},
  volume = {76},
  abstract = {The paper obtains consistent standard errors (SE) and biases of order O(1/n) for the sample standardized regression coefficients with both random and given predictors. Analytical results indicate that the formulas for SEs given in popular text books are consistent only when the population value of the regression coefficient is zero. The sample standardized regression coefficients are also biased in general, although it should not be a concern in practice when the sample size is not too small. Monte Carlo results imply that, for both standardized and unstandardized sample regression coefficients, SE estimates based on asymptotics tend to under-predict the empirical ones at smaller sample sizes.},
  publisher = {Springer Science and Business Media {LLC}},
  keywords = {asymptotics, bias, consistency, Monte Carlo},
  annotation = {standardized-regression, standardized-regression-delta, standardized-regression-normal, standardized-regression-adf},
}

@Article{Yzerbyt-Muller-Batailler-etal-2018,
  author = {Vincent Yzerbyt and Dominique Muller and C{\a'e}dric Batailler and Charles M. Judd},
  date = {2018-12},
  journaltitle = {Journal of Personality and Social Psychology},
  title = {New recommendations for testing indirect effects in mediational models: The need to report and test component paths},
  doi = {10.1037/pspa0000132},
  number = {6},
  pages = {929--943},
  volume = {115},
  abstract = {In light of current concerns with replicability and reporting false-positive effects in psychology, we examine Type I errors and power associated with 2 distinct approaches for the assessment of mediation, namely the component approach (testing individual parameter estimates in the model) and the index approach (testing a single mediational index). We conduct simulations that examine both approaches and show that the most commonly used tests under the index approach risk inflated Type I errors compared with the joint-significance test inspired by the component approach. We argue that the tendency to report only a single mediational index is worrisome for this reason and also because it is often accompanied by a failure to critically examine the individual causal paths underlying the mediational model. We recommend testing individual components of the indirect effect to argue for the presence of an indirect effect and then using other recommended procedures to calculate the size of that effect. Beyond simple mediation, we show that our conclusions also apply in cases of within-participant mediation and moderated mediation. We also provide a new R-package that allows for an easy implementation of our recommendations.},
  publisher = {American Psychological Association ({APA})},
  keywords = {indirect effects, mediation, joint-significance, bootstrap},
  annotation = {mediation, mediation-jointtest},
}

@Article{Zhang-Wang-2012,
  author = {Zhiyong Zhang and Lijuan Wang},
  date = {2012-12},
  journaltitle = {Psychometrika},
  title = {Methods for mediation analysis with missing data},
  doi = {10.1007/s11336-012-9301-5},
  number = {1},
  pages = {154--184},
  volume = {78},
  abstract = {Despite wide applications of both mediation models and missing data techniques, formal discussion of mediation analysis with missing data is still rare. We introduce and compare four approaches to dealing with missing data in mediation analysis including listwise deletion, pairwise deletion, multiple imputation (MI), and a two-stage maximum likelihood (TS-ML) method. An R package bmem is developed to implement the four methods for mediation analysis with missing data in the structural equation modeling framework, and two real examples are used to illustrate the application of the four methods. The four methods are evaluated and compared under MCAR, MAR, and MNAR missing data mechanisms through simulation studies. Both MI and TS-ML perform well for MCAR and MAR data regardless of the inclusion of auxiliary variables and for AV-MNAR data with auxiliary variables. Although listwise deletion and pairwise deletion have low power and large parameter estimation bias in many studied conditions, they may provide useful information for exploring missing mechanisms.},
  publisher = {Springer Science and Business Media {LLC}},
  keywords = {mediation analysis, missing data, MI, TS-ML, bootstrap, auxiliary variables},
  annotation = {mediation, mediation-missing, mediation-bootstrap},
}
